# HCI Cheat Sheet

## Heuristics Checklist
- Visibility of system status: feedback is timely, clear, and meaningful.
- Match between system and real world: language mirrors user mental models.
- User control and freedom: easy undo, cancel, and safe exits.
- Consistency and standards: patterns and terminology stay uniform.
- Error prevention: guardrails reduce slips before they happen.
- Recognition over recall: options are visible and discoverable.
- Flexibility and efficiency: shortcuts for experts without blocking novices.
- Aesthetic and minimalist design: only essential content remains.
- Help users recover from errors: actionable messages and recovery paths.
- Help and documentation: concise guidance when needed.

## Accessibility Quick Checks
- Color contrast meets WCAG AA for text and key UI elements.
- Keyboard navigation covers all interactive controls and focus order is logical.
- Focus states are visible and do not rely on color alone.
- Form fields have labels and errors are announced with guidance.
- Images and icons have meaningful text alternatives.
- Motion can be reduced or disabled; no critical info is conveyed by animation alone.
- Text scales to 200 percent without loss of content or function.
- Headings and landmarks create a clear structure for assistive tech.

## Usability Test Script Template
- Intro: thank participant, explain purpose, set expectations.
- Consent: recording permission, privacy, and right to stop.
- Background: role, experience level, relevant context.
- Tasks: 3 to 5 realistic scenarios with clear success criteria.
- Probing: "What are you thinking?" "What did you expect?"
- Wrap-up: overall impressions, top pain points, suggestions.

## Metrics Definitions
- Task success: percent of participants who complete the task correctly.
- Time-on-task: time required to complete a task (successful attempts only).
- SUS: System Usability Scale, 10-item score from 0 to 100.
- Retention: percent of users returning in a defined time window.

## Experiment Pitfalls
- Changing multiple variables at once without clear attribution.
- Underpowered tests that cannot detect meaningful differences.
- Biased assignment or non-randomized exposure.
- Stopping early based on noisy interim results.
- Optimizing for a metric that conflicts with user goals.
