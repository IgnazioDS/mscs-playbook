# Exploration-Exploitation and Bandits

## Overview
Bandits model decision-making without state transitions, focusing on balancing
exploration and exploitation.

## Why it matters
Exploration strategies apply across RL and are critical for sample efficiency.

## Key ideas
- Multi-armed bandits optimize reward under uncertainty
- Epsilon-greedy is simple but can be inefficient
- UCB balances optimism and uncertainty
- Thompson sampling uses Bayesian sampling

## Practical workflow
- Choose exploration strategy based on risk tolerance
- Track regret and reward over time
- Adjust exploration schedules as learning stabilizes
- Monitor for over-exploration costs

## Failure modes
- Premature exploitation causing suboptimal policies
- Excessive exploration hurting performance
- Poor uncertainty estimates in UCB/Thompson
- Non-stationary rewards break bandit assumptions

## Checklist
- Plot regret curves for exploration strategies
- Tune epsilon or confidence bounds
- Validate on stationary vs non-stationary settings
- Compare against random baselines

## References
- Bandit algorithms — https://arxiv.org/abs/0907.2086
- Thompson sampling — https://proceedings.neurips.cc/paper/2011/hash/2f1661d7ed54faca3b3e9f5dcd9b4f8c-Abstract.html
